
import React, { useState, useRef, useEffect, useCallback } from 'react';
import { GoogleGenAI, LiveServerMessage, Modality } from '@google/genai';
import { TranscriptionEntry, ConnectionStatus } from './types';
import { decode, decodeAudioData, createPcmBlob } from './utils/audio-utils';
import { Visualizer } from './components/Visualizer';

const MODEL_NAME = 'gemini-2.5-flash-native-audio-preview-12-2025';
const SAMPLE_RATE_IN = 16000;
const SAMPLE_RATE_OUT = 24000;

export default function App() {
  const [status, setStatus] = useState<ConnectionStatus>(ConnectionStatus.IDLE);
  const [transcriptions, setTranscriptions] = useState<TranscriptionEntry[]>([]);
  const [isMuted, setIsMuted] = useState(false);
  
  // Audio Refs
  const inputAudioCtxRef = useRef<AudioContext | null>(null);
  const outputAudioCtxRef = useRef<AudioContext | null>(null);
  const outputNodeRef = useRef<GainNode | null>(null);
  const inputNodeRef = useRef<GainNode | null>(null);
  const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());
  const nextStartTimeRef = useRef<number>(0);
  
  // Live Session Refs
  const sessionPromiseRef = useRef<Promise<any> | null>(null);
  const transcriptionBufferRef = useRef({ user: '', model: '' });

  const cleanupAudio = useCallback(() => {
    // Stop all active audio outputs
    sourcesRef.current.forEach(source => {
      try { source.stop(); } catch (e) {}
    });
    sourcesRef.current.clear();
    nextStartTimeRef.current = 0;

    // Close contexts
    if (inputAudioCtxRef.current) {
      inputAudioCtxRef.current.close();
      inputAudioCtxRef.current = null;
    }
    if (outputAudioCtxRef.current) {
      outputAudioCtxRef.current.close();
      outputAudioCtxRef.current = null;
    }
  }, []);

  const handleMessage = async (message: LiveServerMessage) => {
    // Handle Audio Output
    const audioData = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
    if (audioData && outputAudioCtxRef.current && outputNodeRef.current) {
      const ctx = outputAudioCtxRef.current;
      nextStartTimeRef.current = Math.max(nextStartTimeRef.current, ctx.currentTime);
      
      const audioBuffer = await decodeAudioData(
        decode(audioData),
        ctx,
        SAMPLE_RATE_OUT,
        1
      );
      
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(outputNodeRef.current);
      source.addEventListener('ended', () => {
        sourcesRef.current.delete(source);
      });
      
      source.start(nextStartTimeRef.current);
      nextStartTimeRef.current += audioBuffer.duration;
      sourcesRef.current.add(source);
    }

    // Handle Transcriptions
    if (message.serverContent?.inputTranscription) {
      transcriptionBufferRef.current.user += message.serverContent.inputTranscription.text;
    }
    if (message.serverContent?.outputTranscription) {
      transcriptionBufferRef.current.model += message.serverContent.outputTranscription.text;
    }

    // Handle Interruption
    if (message.serverContent?.interrupted) {
      sourcesRef.current.forEach(source => {
        try { source.stop(); } catch (e) {}
      });
      sourcesRef.current.clear();
      nextStartTimeRef.current = 0;
    }

    // Handle Turn Complete
    if (message.serverContent?.turnComplete) {
      const { user, model } = transcriptionBufferRef.current;
      if (user || model) {
        setTranscriptions(prev => [
          ...prev,
          ...(user ? [{ role: 'user' as const, text: user, timestamp: Date.now() }] : []),
          ...(model ? [{ role: 'model' as const, text: model, timestamp: Date.now() }] : [])
        ]);
        transcriptionBufferRef.current = { user: '', model: '' };
      }
    }
  };

  const startSession = async () => {
    if (status !== ConnectionStatus.IDLE) return;
    
    setStatus(ConnectionStatus.CONNECTING);
    try {
      const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
      
      // Initialize Audio Contexts
      inputAudioCtxRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: SAMPLE_RATE_IN });
      outputAudioCtxRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: SAMPLE_RATE_OUT });
      
      outputNodeRef.current = outputAudioCtxRef.current.createGain();
      outputNodeRef.current.connect(outputAudioCtxRef.current.destination);
      
      inputNodeRef.current = inputAudioCtxRef.current.createGain();

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      const sessionPromise = ai.live.connect({
        model: MODEL_NAME,
        callbacks: {
          onopen: () => {
            setStatus(ConnectionStatus.CONNECTED);
            
            // Start streaming microphone
            const source = inputAudioCtxRef.current!.createMediaStreamSource(stream);
            const scriptProcessor = inputAudioCtxRef.current!.createScriptProcessor(4096, 1, 1);
            
            scriptProcessor.onaudioprocess = (e) => {
              if (isMuted) return;
              const inputData = e.inputBuffer.getChannelData(0);
              const pcmBlob = createPcmBlob(inputData);
              
              sessionPromise.then(session => {
                session.sendRealtimeInput({ media: pcmBlob });
              });
            };
            
            source.connect(scriptProcessor);
            scriptProcessor.connect(inputAudioCtxRef.current!.destination);
          },
          onmessage: handleMessage,
          onerror: (err) => {
            console.error('Session Error:', err);
            setStatus(ConnectionStatus.ERROR);
            cleanupAudio();
          },
          onclose: () => {
            setStatus(ConnectionStatus.IDLE);
            cleanupAudio();
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: "Sei un assistente vocale italiano intelligente, amichevole e sintetico. Rispondi in modo naturale e coinvolgente.",
          speechConfig: {
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Kore' } },
          },
          inputAudioTranscription: {},
          outputAudioTranscription: {},
        }
      });

      sessionPromiseRef.current = sessionPromise;

    } catch (err) {
      console.error('Failed to start session:', err);
      setStatus(ConnectionStatus.ERROR);
      cleanupAudio();
    }
  };

  const stopSession = async () => {
    if (sessionPromiseRef.current) {
      const session = await sessionPromiseRef.current;
      session.close();
      sessionPromiseRef.current = null;
    }
    cleanupAudio();
    setStatus(ConnectionStatus.IDLE);
  };

  const toggleMute = () => setIsMuted(!isMuted);

  return (
    <div className="min-h-screen flex flex-col p-4 md:p-8 relative">
      {/* Background decoration */}
      <div className="absolute top-0 left-0 w-full h-full overflow-hidden pointer-events-none -z-10">
          <div className="absolute -top-1/4 -left-1/4 w-1/2 h-1/2 bg-blue-500/10 blur-[120px] rounded-full"></div>
          <div className="absolute -bottom-1/4 -right-1/4 w-1/2 h-1/2 bg-indigo-500/10 blur-[120px] rounded-full"></div>
      </div>

      {/* Header */}
      <header className="flex justify-between items-center mb-8 glass-panel px-6 py-4 rounded-2xl">
        <div className="flex items-center gap-3">
          <div className={`w-3 h-3 rounded-full ${
            status === ConnectionStatus.CONNECTED ? 'bg-green-500 animate-pulse' : 
            status === ConnectionStatus.CONNECTING ? 'bg-yellow-500 animate-pulse' : 
            status === ConnectionStatus.ERROR ? 'bg-red-500' : 'bg-gray-500'
          }`} />
          <h1 className="text-xl font-bold tracking-tight bg-gradient-to-r from-blue-400 to-indigo-400 bg-clip-text text-transparent">
            Gemini Live Agent
          </h1>
        </div>
        <div className="text-xs text-gray-400 font-mono hidden md:block">
          {MODEL_NAME}
        </div>
      </header>

      {/* Main UI */}
      <main className="flex-1 flex flex-col md:flex-row gap-6 overflow-hidden">
        
        {/* Left: Visualization & Controls */}
        <section className="flex-1 glass-panel rounded-3xl p-8 flex flex-col items-center justify-center relative min-h-[400px]">
          <Visualizer 
            isActive={status === ConnectionStatus.CONNECTED} 
            audioContext={outputAudioCtxRef.current} 
            gainNode={outputNodeRef.current} 
          />
          
          <div className="mt-8 flex flex-col items-center gap-6 w-full max-w-sm">
            <div className="text-center">
              <p className="text-sm font-medium text-gray-400 uppercase tracking-widest mb-1">
                {status === ConnectionStatus.CONNECTED ? 'Listening & Speaking' : status}
              </p>
              <h2 className="text-2xl font-semibold">
                {status === ConnectionStatus.CONNECTED ? 'Conversazione Attiva' : 'Pronto per iniziare'}
              </h2>
            </div>

            <div className="flex gap-4 w-full">
              {status === ConnectionStatus.IDLE || status === ConnectionStatus.ERROR ? (
                <button 
                  onClick={startSession}
                  className="flex-1 bg-white text-black hover:bg-gray-200 font-bold py-4 px-8 rounded-2xl transition-all shadow-xl shadow-white/5 flex items-center justify-center gap-2"
                >
                  <PlayIcon /> Inizia Chiamata
                </button>
              ) : (
                <>
                  <button 
                    onClick={toggleMute}
                    className={`flex-1 py-4 px-4 rounded-2xl transition-all font-bold flex items-center justify-center gap-2 ${
                        isMuted ? 'bg-red-500/20 text-red-400 border border-red-500/30' : 'bg-gray-800 text-white border border-white/10'
                    }`}
                  >
                    {isMuted ? <MicOffIcon /> : <MicIcon />} {isMuted ? 'Muto' : 'Microfono'}
                  </button>
                  <button 
                    onClick={stopSession}
                    className="flex-1 bg-red-600 hover:bg-red-700 text-white font-bold py-4 px-8 rounded-2xl transition-all shadow-xl shadow-red-900/20 flex items-center justify-center gap-2"
                  >
                    <StopIcon /> Termina
                  </button>
                </>
              )}
            </div>
          </div>
        </section>

        {/* Right: Transcription */}
        <section className="w-full md:w-96 glass-panel rounded-3xl p-6 flex flex-col overflow-hidden">
          <h3 className="text-sm font-bold text-gray-500 uppercase tracking-widest mb-4 flex items-center gap-2">
            <ListIcon /> Trascrizione Real-time
          </h3>
          <div className="flex-1 overflow-y-auto space-y-4 pr-2 custom-scrollbar">
            {transcriptions.length === 0 && (
              <div className="h-full flex items-center justify-center text-gray-500 text-sm italic text-center px-8">
                I messaggi appariranno qui man mano che parli...
              </div>
            )}
            {transcriptions.map((entry, i) => (
              <div 
                key={i} 
                className={`flex flex-col ${entry.role === 'user' ? 'items-end' : 'items-start'}`}
              >
                <div className={`max-w-[85%] p-3 rounded-2xl text-sm ${
                  entry.role === 'user' 
                    ? 'bg-blue-600 text-white rounded-tr-none' 
                    : 'bg-white/10 text-gray-200 rounded-tl-none border border-white/5'
                }`}>
                  {entry.text}
                </div>
                <span className="text-[10px] text-gray-500 mt-1 uppercase">
                  {entry.role === 'user' ? 'Tu' : 'Gemini'}
                </span>
              </div>
            ))}
          </div>
        </section>
      </main>

      {/* Footer */}
      <footer className="mt-8 text-center text-gray-500 text-xs">
        Powered by Google Gemini 2.5 Flash Native Audio â€¢ Advanced Real-time Interface
      </footer>
    </div>
  );
}

// Minimal Icons
const PlayIcon = () => <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"/></svg>;
const StopIcon = () => <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><rect x="6" y="6" width="12" height="12" rx="2"/></svg>;
const MicIcon = () => <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"/><path d="M19 10v2a7 7 0 0 1-14 0v-2"/><line x1="12" x2="12" y1="19" y2="22"/></svg>;
const MicOffIcon = () => <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><line x1="2" x2="22" y1="2" y2="22"/><path d="M18.89 13.23A7.12 7.12 0 0 0 19 12v-2"/><path d="M5 10v2a7 7 0 0 0 12 5"/><path d="M15 9.34V5a3 3 0 0 0-5.68-1.33"/><path d="M9 9v3a3 3 0 0 0 5.12 2.12"/><line x1="12" x2="12" y1="19" y2="22"/></svg>;
const ListIcon = () => <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><line x1="8" y1="6" x2="21" y2="6"/><line x1="8" y1="12" x2="21" y2="12"/><line x1="8" y1="18" x2="21" y2="18"/><line x1="3" y1="6" x2="3.01" y2="6"/><line x1="3" y1="12" x2="3.01" y2="12"/><line x1="3" y1="18" x2="3.01" y2="18"/></svg>;
